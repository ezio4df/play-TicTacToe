{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:02:20.784937647Z",
     "start_time": "2026-01-01T12:02:20.727754075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "from typing import Tuple, Dict, Optional\n",
    "import random\n",
    "import os"
   ],
   "id": "655fbc7b8b473c4b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:02:21.445768144Z",
     "start_time": "2026-01-01T12:02:21.420525025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GomokuEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [], \"render_fps\": 1}\n",
    "\n",
    "    def __init__(self, board_size: int = 15, agent_is_x: bool = True):\n",
    "        self.board_size = board_size\n",
    "        self.agent_is_x = agent_is_x\n",
    "        self.board = np.zeros((board_size, board_size), dtype=int)\n",
    "        self.action_space = gym.spaces.Discrete(board_size * board_size)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-1, high=1, shape=(board_size, board_size), dtype=int\n",
    "        )\n",
    "\n",
    "    def _count(self, player: int, r: int, c: int, dr: int, dc: int) -> int:\n",
    "        count = 0\n",
    "        nr, nc = r + dr, c + dc\n",
    "        while 0 <= nr < self.board_size and 0 <= nc < self.board_size and self.board[nr, nc] == player:\n",
    "            count += 1\n",
    "            nr += dr\n",
    "            nc += dc\n",
    "        return count\n",
    "\n",
    "    def _check_winner_from_move(self, r: int, c: int) -> int:\n",
    "        player = self.board[r, c]\n",
    "        if player == 0:\n",
    "            return 0\n",
    "        for dr, dc in [(0,1), (1,0), (1,1), (1,-1)]:\n",
    "            total = 1 + self._count(player, r, c, dr, dc) + self._count(player, r, c, -dr, -dc)\n",
    "            if total >= 5:\n",
    "                return player\n",
    "        return 0\n",
    "\n",
    "    def _is_full(self) -> bool:\n",
    "        return not (self.board == 0).any()\n",
    "\n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        return self.board.copy() if self.agent_is_x else -self.board.copy()\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None) -> Tuple[np.ndarray, dict]:\n",
    "        super().reset(seed=seed)\n",
    "        self.board = np.zeros((self.board_size, self.board_size), dtype=int)\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, dict]:\n",
    "        if not (0 <= action < self.board_size * self.board_size):\n",
    "            obs = self._get_obs()\n",
    "            return obs, -10.0, True, False, {}\n",
    "\n",
    "        r, c = divmod(action, self.board_size)\n",
    "        if self.board[r, c] != 0:\n",
    "            obs = self._get_obs()\n",
    "            return obs, -10.0, True, False, {}\n",
    "\n",
    "        player_symbol = 1 if self.agent_is_x else -1\n",
    "        self.board[r, c] = player_symbol\n",
    "        winner = self._check_winner_from_move(r, c)\n",
    "\n",
    "        if winner == player_symbol:\n",
    "            obs = self._get_obs()\n",
    "            return obs, 1.0, True, False, {}\n",
    "        if self._is_full():\n",
    "            obs = self._get_obs()\n",
    "            return obs, 0.0, True, False, {}\n",
    "\n",
    "        opp_symbol = -1 if self.agent_is_x else 1\n",
    "        valid = np.argwhere(self.board == 0)\n",
    "        if len(valid) == 0:\n",
    "            obs = self._get_obs()\n",
    "            return obs, 0.0, True, False, {}\n",
    "\n",
    "        idx = self.np_random.integers(len(valid))\n",
    "        orow, ocol = valid[idx]\n",
    "        self.board[orow, ocol] = opp_symbol\n",
    "        winner = self._check_winner_from_move(orow, ocol)\n",
    "\n",
    "        if winner == opp_symbol:\n",
    "            reward = -1.0\n",
    "            terminated = True\n",
    "        elif self._is_full():\n",
    "            reward = 0.0\n",
    "            terminated = True\n",
    "        else:\n",
    "            reward = 0.0\n",
    "            terminated = False\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        return obs, reward, terminated, False, {}\n",
    "\n",
    "    def get_valid_mask(self) -> np.ndarray:\n",
    "        return (self.board.flatten() == 0)\n"
   ],
   "id": "8d9e2ff5996cb048",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:02:21.643912401Z",
     "start_time": "2026-01-01T12:02:21.627401437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, board_size: int = 15, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.board_size = board_size\n",
    "        self.input_dim = board_size * board_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, self.input_dim)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x).squeeze(-1)\n",
    "        return logits, value"
   ],
   "id": "4a4e529e420ea055",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:02:21.867185452Z",
     "start_time": "2026-01-01T12:02:21.836025512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SelfPlayA2C:\n",
    "    def __init__(\n",
    "        self,\n",
    "        board_size: int = 15,\n",
    "        lr: float = 1e-4,\n",
    "        gamma: float = 0.99,\n",
    "        entropy_coef: float = 0.01,\n",
    "        value_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        past_agents_window: int = 5\n",
    "    ):\n",
    "        self.board_size = board_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.gamma = gamma\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.policy = PolicyNetwork(board_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        self.past_policies = deque(maxlen=past_agents_window)\n",
    "\n",
    "    def get_action(self, state: np.ndarray, valid_mask: np.ndarray):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        valid_mask_tensor = torch.tensor(valid_mask, dtype=torch.bool).unsqueeze(0).to(self.device)\n",
    "\n",
    "        logits, value = self.policy(state_tensor)\n",
    "        masked_logits = logits.masked_fill(~valid_mask_tensor, -1e9)\n",
    "        probs = torch.softmax(masked_logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "\n",
    "        return action.item(), log_prob, value, entropy\n",
    "\n",
    "    def compute_returns(self, rewards, dones, final_value, gamma):\n",
    "        returns = []\n",
    "        R = final_value\n",
    "        for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "            R = r + gamma * R * (1 - d)\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def train_epoch(self, num_episodes: int = 100):\n",
    "        total_loss = 0.0\n",
    "        total_episodes = 0\n",
    "\n",
    "        for _ in range(num_episodes):\n",
    "            # Select opponent: 50% current, 50% past\n",
    "            if len(self.past_policies) > 0 and random.random() < 0.5:\n",
    "                opponent = random.choice(self.past_policies)\n",
    "                agent_is_x = random.choice([True, False])\n",
    "            else:\n",
    "                opponent = None\n",
    "                agent_is_x = True\n",
    "\n",
    "            env = GomokuEnv(board_size=self.board_size, agent_is_x=agent_is_x)\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "            entropies = []\n",
    "            dones_list = []\n",
    "\n",
    "            while not done:\n",
    "                valid_mask = env.get_valid_mask()\n",
    "                action, log_prob, value, entropy = self.get_action(state, valid_mask)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                rewards.append(reward)\n",
    "                entropies.append(entropy)\n",
    "                dones_list.append(float(done))\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Bootstrap final value\n",
    "            if done:\n",
    "                final_value = 0.0\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                _, final_value = self.policy(state_tensor)\n",
    "                final_value = final_value.item()\n",
    "\n",
    "            returns = self.compute_returns(rewards, dones_list, final_value, self.gamma)\n",
    "            returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "            values = torch.cat(values)\n",
    "            advantages = returns - values\n",
    "\n",
    "            log_probs = torch.cat(log_probs)\n",
    "            entropies = torch.cat(entropies)\n",
    "\n",
    "            actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "            critic_loss = advantages.pow(2).mean()\n",
    "            entropy_loss = -entropies.mean()\n",
    "\n",
    "            loss = actor_loss + self.value_coef * critic_loss + self.entropy_coef * entropy_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_episodes += 1\n",
    "\n",
    "        return total_loss / total_episodes\n",
    "\n",
    "    def save_policy(self, path: str):\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "\n",
    "    def add_past_policy(self):\n",
    "        self.past_policies.append(self._copy_policy())\n",
    "\n",
    "    def _copy_policy(self):\n",
    "        policy_copy = PolicyNetwork(self.board_size).to(self.device)\n",
    "        policy_copy.load_state_dict(self.policy.state_dict())\n",
    "        policy_copy.eval()\n",
    "        return policy_copy"
   ],
   "id": "901a885bf2c77f6b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:02:22.897938647Z",
     "start_time": "2026-01-01T12:02:22.865202402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_policy(policy, board_size: int = 15, episodes: int = 100):\n",
    "    wins, draws, losses = 0, 0, 0\n",
    "    device = next(policy.parameters()).device\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        agent_is_x = random.choice([True, False])\n",
    "        env = GomokuEnv(board_size=board_size, agent_is_x=agent_is_x)\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            valid_mask = env.get_valid_mask()\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            valid_mask_tensor = torch.tensor(valid_mask, dtype=torch.bool).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, _ = policy(state_tensor)\n",
    "                masked_logits = logits.masked_fill(~valid_mask_tensor, -1e9)\n",
    "                action = masked_logits.argmax(dim=-1).item()\n",
    "\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "        if reward == 1.0:\n",
    "            wins += 1\n",
    "        elif reward == 0.0:\n",
    "            draws += 1\n",
    "        else:\n",
    "            losses += 1\n",
    "\n",
    "    return wins / episodes, draws / episodes, losses / episodes"
   ],
   "id": "5ae43cda31bbd341",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:23:36.403136596Z",
     "start_time": "2026-01-01T12:02:23.342942534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = SelfPlayA2C(board_size=15, lr=1e-4)\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss = trainer.train_epoch(num_episodes=50)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        win_rate, draw_rate, loss_rate = evaluate_policy(trainer.policy, episodes=100)\n",
    "        print(f\"Epoch {epoch}: Loss={loss:.4f}, Win={win_rate:.2%}, Draw={draw_rate:.2%}, Loss={loss_rate:.2%}\")\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        trainer.add_past_policy()\n",
    "        trainer.save_policy(f\"models/gomoku_a2c_epoch_{epoch}.pth\")\n",
    "\n",
    "trainer.save_policy(\"models/gomoku_a2c_final.pth\")"
   ],
   "id": "80b44e9da3a299e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=0.3539, Win=73.00%, Draw=0.00%, Loss=27.00%\n",
      "Epoch 10: Loss=-1.0031, Win=84.00%, Draw=0.00%, Loss=16.00%\n",
      "Epoch 20: Loss=-1.8752, Win=84.00%, Draw=0.00%, Loss=16.00%\n",
      "Epoch 30: Loss=-0.9925, Win=72.00%, Draw=0.00%, Loss=28.00%\n",
      "Epoch 40: Loss=-1.7241, Win=83.00%, Draw=0.00%, Loss=17.00%\n",
      "Epoch 50: Loss=-2.2033, Win=94.00%, Draw=0.00%, Loss=6.00%\n",
      "Epoch 60: Loss=-2.8177, Win=94.00%, Draw=0.00%, Loss=6.00%\n",
      "Epoch 70: Loss=-2.6034, Win=97.00%, Draw=0.00%, Loss=3.00%\n",
      "Epoch 80: Loss=-1.6202, Win=98.00%, Draw=0.00%, Loss=2.00%\n",
      "Epoch 90: Loss=-1.6863, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 100: Loss=-1.3080, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 110: Loss=-1.3425, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 120: Loss=-1.8941, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 130: Loss=-2.0292, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 140: Loss=-1.4403, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 150: Loss=-2.1235, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 160: Loss=-0.6771, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 170: Loss=-0.9645, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 180: Loss=-0.6218, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 190: Loss=-0.7695, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 200: Loss=-0.7618, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 210: Loss=-0.4770, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 220: Loss=-0.2599, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 230: Loss=-0.6949, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 240: Loss=-0.3808, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 250: Loss=-0.0657, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 260: Loss=-0.0300, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 270: Loss=-0.4334, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 280: Loss=-0.2560, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 290: Loss=-0.2904, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 300: Loss=-0.7704, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 310: Loss=-0.4329, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 320: Loss=-0.7224, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 330: Loss=-0.5053, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 340: Loss=-0.1413, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 350: Loss=-0.2364, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 360: Loss=-0.5123, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 370: Loss=-0.1461, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 380: Loss=-0.2288, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 390: Loss=-0.1624, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 400: Loss=-0.4287, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 410: Loss=-0.0602, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 420: Loss=-0.0786, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 430: Loss=-0.0593, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 440: Loss=-0.4044, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 450: Loss=-0.4708, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 460: Loss=-0.4253, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 470: Loss=-0.6377, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 480: Loss=-0.7816, Win=99.00%, Draw=0.00%, Loss=1.00%\n",
      "Epoch 490: Loss=-0.1874, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 500: Loss=-0.5948, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 510: Loss=-0.4392, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 520: Loss=-0.2191, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 530: Loss=-0.1587, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 540: Loss=-0.4331, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 550: Loss=-0.4329, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 560: Loss=-0.0269, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 570: Loss=-0.4580, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 580: Loss=-0.6594, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 590: Loss=-0.3936, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 600: Loss=-0.3373, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 610: Loss=-0.0398, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 620: Loss=-0.2549, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 630: Loss=-0.1979, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 640: Loss=-0.4121, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 650: Loss=-0.2855, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 660: Loss=-0.1822, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 670: Loss=-0.1772, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 680: Loss=-0.1772, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 690: Loss=-0.0522, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 700: Loss=-0.6828, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 710: Loss=-0.3189, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 720: Loss=-0.0761, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 730: Loss=-0.1600, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 740: Loss=-0.0495, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 750: Loss=-0.2003, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 760: Loss=-0.4427, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 770: Loss=-0.4730, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 780: Loss=-0.0817, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 790: Loss=-0.4229, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 800: Loss=-0.3129, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 810: Loss=-0.2609, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 820: Loss=-0.0440, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 830: Loss=-0.4471, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 840: Loss=-0.3046, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 850: Loss=-0.3147, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 860: Loss=-0.0887, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 870: Loss=-0.2969, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 880: Loss=-0.4340, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 890: Loss=-0.0512, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 900: Loss=-0.0735, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 910: Loss=-0.2447, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 920: Loss=-0.3213, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 930: Loss=-0.0628, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 940: Loss=-0.2170, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 950: Loss=-0.1367, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 960: Loss=-0.1165, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 970: Loss=-0.1192, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 980: Loss=-0.0682, Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Epoch 990: Loss=-0.5266, Win=100.00%, Draw=0.00%, Loss=0.00%\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Play against the bot",
   "id": "4a8330b74e98f969"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T13:13:25.704957594Z",
     "start_time": "2026-01-01T13:09:07.816323170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "BOARD_SIZE = 15\n",
    "CELL_SIZE = 30  # pixels\n",
    "WINDOW_SIZE = BOARD_SIZE * CELL_SIZE\n",
    "MODEL_PATH = \"models/gomoku_a2c_final.pth\"\n",
    "\n",
    "# ----------------------------\n",
    "# Load agent\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy = PolicyNetwork(board_size=BOARD_SIZE).to(device)\n",
    "policy.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "policy.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# Game state\n",
    "# ----------------------------\n",
    "env = GomokuEnv(board_size=BOARD_SIZE, agent_is_x=True)\n",
    "obs, _ = env.reset()\n",
    "game_over = False\n",
    "message = \"Click to place O (you play second). Press R to reset, Q to quit.\"\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: check win\n",
    "# ----------------------------\n",
    "def check_win(board, r, c):\n",
    "    if board[r, c] == 0:\n",
    "        return 0\n",
    "    player = board[r, c]\n",
    "    for dr, dc in [(0,1), (1,0), (1,1), (1,-1)]:\n",
    "        count = 1\n",
    "        for sign in [1, -1]:\n",
    "            nr, nc = r + sign*dr, c + sign*dc\n",
    "            while 0 <= nr < BOARD_SIZE and 0 <= nc < BOARD_SIZE and board[nr, nc] == player:\n",
    "                count += 1\n",
    "                nr += sign*dr\n",
    "                nc += sign*dc\n",
    "        if count >= 5:\n",
    "            return player\n",
    "    return 0\n",
    "\n",
    "# ----------------------------\n",
    "# Draw board\n",
    "# ----------------------------\n",
    "def draw_board(img, board, message):\n",
    "    img[:] = 255  # white background\n",
    "\n",
    "    # Draw grid\n",
    "    for i in range(1, BOARD_SIZE):\n",
    "        cv2.line(img, (i * CELL_SIZE, 0), (i * CELL_SIZE, WINDOW_SIZE), (200, 200, 200), 1)\n",
    "        cv2.line(img, (0, i * CELL_SIZE), (WINDOW_SIZE, i * CELL_SIZE), (200, 200, 200), 1)\n",
    "\n",
    "    # Draw stones\n",
    "    for i in range(BOARD_SIZE):\n",
    "        for j in range(BOARD_SIZE):\n",
    "            center = (j * CELL_SIZE + CELL_SIZE // 2, i * CELL_SIZE + CELL_SIZE // 2)\n",
    "            if board[i, j] == 1:  # X (agent)\n",
    "                cv2.circle(img, center, CELL_SIZE // 2 - 2, (0, 0, 0), -1)\n",
    "            elif board[i, j] == -1:  # O (you)\n",
    "                cv2.circle(img, center, CELL_SIZE // 2 - 2, (255, 255, 255), -1)\n",
    "                cv2.circle(img, center, CELL_SIZE // 2 - 2, (0, 0, 0), 1)\n",
    "\n",
    "    # Draw message\n",
    "    cv2.putText(img, message, (10, WINDOW_SIZE + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
    "\n",
    "# ----------------------------\n",
    "# Main loop\n",
    "# ----------------------------\n",
    "img = np.ones((WINDOW_SIZE + 30, WINDOW_SIZE, 3), dtype=np.uint8) * 255\n",
    "cv2.namedWindow(\"Gomoku - Play vs A2C Agent\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global obs, game_over, message\n",
    "    if event == cv2.EVENT_LBUTTONDOWN and not game_over:\n",
    "        if y >= WINDOW_SIZE:  # clicked on message area\n",
    "            return\n",
    "        col = x // CELL_SIZE\n",
    "        row = y // CELL_SIZE\n",
    "        if not (0 <= row < BOARD_SIZE and 0 <= col < BOARD_SIZE):\n",
    "            return\n",
    "        if env.board[row, col] != 0:\n",
    "            message = \"Invalid: cell occupied.\"\n",
    "            return\n",
    "\n",
    "        # Human move\n",
    "        env.board[row, col] = -1\n",
    "        winner = check_win(env.board, row, col)\n",
    "        full = not (env.board == 0).any()\n",
    "\n",
    "        if winner == -1:\n",
    "            message = \"You win! Press R to reset.\"\n",
    "            game_over = True\n",
    "        elif full:\n",
    "            message = \"Draw! Press R to reset.\"\n",
    "            game_over = True\n",
    "        else:\n",
    "            # Agent move\n",
    "            valid_mask = (env.board.flatten() == 0)\n",
    "            state_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            valid_mask_tensor = torch.tensor(valid_mask, dtype=torch.bool).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                logits, _ = policy(state_tensor)\n",
    "                masked_logits = logits.masked_fill(~valid_mask_tensor, -1e9)\n",
    "                action = masked_logits.argmax(dim=-1).item()\n",
    "            ar, ac = divmod(action, BOARD_SIZE)\n",
    "            env.board[ar, ac] = 1\n",
    "            obs = env._get_obs()\n",
    "            winner = check_win(env.board, ar, ac)\n",
    "            full = not (env.board == 0).any()\n",
    "            if winner == 1:\n",
    "                message = \"Agent wins! Press R to reset.\"\n",
    "                game_over = True\n",
    "            elif full:\n",
    "                message = \"Draw! Press R to reset.\"\n",
    "                game_over = True\n",
    "            else:\n",
    "                message = \"Your turn: click to place O.\"\n",
    "\n",
    "        draw_board(img, env.board, message)\n",
    "        cv2.imshow(\"Gomoku - Play vs A2C Agent\", img)\n",
    "\n",
    "cv2.setMouseCallback(\"Gomoku - Play vs A2C Agent\", mouse_callback)\n",
    "draw_board(img, env.board, message)\n",
    "cv2.imshow(\"Gomoku - Play vs A2C Agent\", img)\n",
    "\n",
    "print(\"Game started. Click to play. Press R to reset, Q to quit.\")\n",
    "\n",
    "while True:\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q') or cv2.getWindowProperty(\"Gomoku - Play vs A2C Agent\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "    if key == ord('r'):\n",
    "        env.reset()\n",
    "        obs, _ = env.reset()\n",
    "        game_over = False\n",
    "        message = \"Click to place O (you play second).\"\n",
    "        draw_board(img, env.board, message)\n",
    "        cv2.imshow(\"Gomoku - Play vs A2C Agent\", img)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "f0a63b3d4509ecc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game started. Click to play. Press R to reset, Q to quit.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T13:16:24.311413173Z",
     "start_time": "2026-01-01T13:16:24.285727002Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f2993ff86ee96c32",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T13:16:49.719060567Z",
     "start_time": "2026-01-01T13:16:47.679470603Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a1a6c0551a36b9c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Opponent:   Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Heuristic Opponent: Win=100.00%, Draw=0.00%, Loss=0.00%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PolicyNetwork:\n\tMissing key(s) in state_dict: \"actor.1.weight\", \"actor.1.bias\", \"actor.3.weight\", \"actor.3.bias\", \"actor.5.weight\", \"actor.5.bias\", \"critic.1.weight\", \"critic.1.bias\", \"critic.3.weight\", \"critic.3.bias\", \"critic.5.weight\", \"critic.5.bias\". \n\tUnexpected key(s) in state_dict: \"net.1.weight\", \"net.1.bias\", \"net.3.weight\", \"net.3.bias\", \"net.5.weight\", \"net.5.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mrun_full_evaluation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpolicy_path\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodels/gomoku_a2c_final.pth\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_models_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodels\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mboard_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m15\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepisodes_per_match\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m100\u001B[39;49m\n\u001B[32m      6\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[34]\u001B[39m\u001B[32m, line 217\u001B[39m, in \u001B[36mrun_full_evaluation\u001B[39m\u001B[34m(policy_path, past_models_dir, board_size, episodes_per_match, device)\u001B[39m\n\u001B[32m    215\u001B[39m \u001B[38;5;66;03m# Load opponent policy\u001B[39;00m\n\u001B[32m    216\u001B[39m opp_policy = PolicyNetwork(board_size=board_size).to(device)\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m \u001B[43mopp_policy\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpast_models_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_file\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    218\u001B[39m opp_policy.eval()\n\u001B[32m    220\u001B[39m \u001B[38;5;66;03m# Play matches\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/play-TicTacToe/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2629\u001B[39m, in \u001B[36mModule.load_state_dict\u001B[39m\u001B[34m(self, state_dict, strict, assign)\u001B[39m\n\u001B[32m   2621\u001B[39m         error_msgs.insert(\n\u001B[32m   2622\u001B[39m             \u001B[32m0\u001B[39m,\n\u001B[32m   2623\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m. \u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m   2624\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m.join(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)\n\u001B[32m   2625\u001B[39m             ),\n\u001B[32m   2626\u001B[39m         )\n\u001B[32m   2628\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) > \u001B[32m0\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m2629\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   2630\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(\n\u001B[32m   2631\u001B[39m             \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m, \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[33m\"\u001B[39m.join(error_msgs)\n\u001B[32m   2632\u001B[39m         )\n\u001B[32m   2633\u001B[39m     )\n\u001B[32m   2634\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[31mRuntimeError\u001B[39m: Error(s) in loading state_dict for PolicyNetwork:\n\tMissing key(s) in state_dict: \"actor.1.weight\", \"actor.1.bias\", \"actor.3.weight\", \"actor.3.bias\", \"actor.5.weight\", \"actor.5.bias\", \"critic.1.weight\", \"critic.1.bias\", \"critic.3.weight\", \"critic.3.bias\", \"critic.5.weight\", \"critic.5.bias\". \n\tUnexpected key(s) in state_dict: \"net.1.weight\", \"net.1.bias\", \"net.3.weight\", \"net.3.bias\", \"net.5.weight\", \"net.5.bias\". "
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c6d1f878d5f561a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
