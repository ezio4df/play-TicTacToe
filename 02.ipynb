{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:02:20.784937647Z",
     "start_time": "2026-01-01T12:02:20.727754075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "from typing import Tuple, Dict, Optional\n",
    "import random\n",
    "import os"
   ],
   "id": "655fbc7b8b473c4b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:02:21.445768144Z",
     "start_time": "2026-01-01T12:02:21.420525025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GomokuEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [], \"render_fps\": 1}\n",
    "\n",
    "    def __init__(self, board_size: int = 15, agent_is_x: bool = True):\n",
    "        self.board_size = board_size\n",
    "        self.agent_is_x = agent_is_x\n",
    "        self.board = np.zeros((board_size, board_size), dtype=int)\n",
    "        self.action_space = gym.spaces.Discrete(board_size * board_size)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-1, high=1, shape=(board_size, board_size), dtype=int\n",
    "        )\n",
    "\n",
    "    def _count(self, player: int, r: int, c: int, dr: int, dc: int) -> int:\n",
    "        count = 0\n",
    "        nr, nc = r + dr, c + dc\n",
    "        while 0 <= nr < self.board_size and 0 <= nc < self.board_size and self.board[nr, nc] == player:\n",
    "            count += 1\n",
    "            nr += dr\n",
    "            nc += dc\n",
    "        return count\n",
    "\n",
    "    def _check_winner_from_move(self, r: int, c: int) -> int:\n",
    "        player = self.board[r, c]\n",
    "        if player == 0:\n",
    "            return 0\n",
    "        for dr, dc in [(0,1), (1,0), (1,1), (1,-1)]:\n",
    "            total = 1 + self._count(player, r, c, dr, dc) + self._count(player, r, c, -dr, -dc)\n",
    "            if total >= 5:\n",
    "                return player\n",
    "        return 0\n",
    "\n",
    "    def _is_full(self) -> bool:\n",
    "        return not (self.board == 0).any()\n",
    "\n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        return self.board.copy() if self.agent_is_x else -self.board.copy()\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None) -> Tuple[np.ndarray, dict]:\n",
    "        super().reset(seed=seed)\n",
    "        self.board = np.zeros((self.board_size, self.board_size), dtype=int)\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, dict]:\n",
    "        if not (0 <= action < self.board_size * self.board_size):\n",
    "            obs = self._get_obs()\n",
    "            return obs, -10.0, True, False, {}\n",
    "\n",
    "        r, c = divmod(action, self.board_size)\n",
    "        if self.board[r, c] != 0:\n",
    "            obs = self._get_obs()\n",
    "            return obs, -10.0, True, False, {}\n",
    "\n",
    "        player_symbol = 1 if self.agent_is_x else -1\n",
    "        self.board[r, c] = player_symbol\n",
    "        winner = self._check_winner_from_move(r, c)\n",
    "\n",
    "        if winner == player_symbol:\n",
    "            obs = self._get_obs()\n",
    "            return obs, 1.0, True, False, {}\n",
    "        if self._is_full():\n",
    "            obs = self._get_obs()\n",
    "            return obs, 0.0, True, False, {}\n",
    "\n",
    "        opp_symbol = -1 if self.agent_is_x else 1\n",
    "        valid = np.argwhere(self.board == 0)\n",
    "        if len(valid) == 0:\n",
    "            obs = self._get_obs()\n",
    "            return obs, 0.0, True, False, {}\n",
    "\n",
    "        idx = self.np_random.integers(len(valid))\n",
    "        orow, ocol = valid[idx]\n",
    "        self.board[orow, ocol] = opp_symbol\n",
    "        winner = self._check_winner_from_move(orow, ocol)\n",
    "\n",
    "        if winner == opp_symbol:\n",
    "            reward = -1.0\n",
    "            terminated = True\n",
    "        elif self._is_full():\n",
    "            reward = 0.0\n",
    "            terminated = True\n",
    "        else:\n",
    "            reward = 0.0\n",
    "            terminated = False\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        return obs, reward, terminated, False, {}\n",
    "\n",
    "    def get_valid_mask(self) -> np.ndarray:\n",
    "        return (self.board.flatten() == 0)\n"
   ],
   "id": "8d9e2ff5996cb048",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:02:21.643912401Z",
     "start_time": "2026-01-01T12:02:21.627401437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, board_size: int = 15, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.board_size = board_size\n",
    "        self.input_dim = board_size * board_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, self.input_dim)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x).squeeze(-1)\n",
    "        return logits, value"
   ],
   "id": "4a4e529e420ea055",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:02:21.867185452Z",
     "start_time": "2026-01-01T12:02:21.836025512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SelfPlayA2C:\n",
    "    def __init__(\n",
    "        self,\n",
    "        board_size: int = 15,\n",
    "        lr: float = 1e-4,\n",
    "        gamma: float = 0.99,\n",
    "        entropy_coef: float = 0.01,\n",
    "        value_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        past_agents_window: int = 5\n",
    "    ):\n",
    "        self.board_size = board_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.gamma = gamma\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        self.policy = PolicyNetwork(board_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        self.past_policies = deque(maxlen=past_agents_window)\n",
    "\n",
    "    def get_action(self, state: np.ndarray, valid_mask: np.ndarray):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        valid_mask_tensor = torch.tensor(valid_mask, dtype=torch.bool).unsqueeze(0).to(self.device)\n",
    "\n",
    "        logits, value = self.policy(state_tensor)\n",
    "        masked_logits = logits.masked_fill(~valid_mask_tensor, -1e9)\n",
    "        probs = torch.softmax(masked_logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "\n",
    "        return action.item(), log_prob, value, entropy\n",
    "\n",
    "    def compute_returns(self, rewards, dones, final_value, gamma):\n",
    "        returns = []\n",
    "        R = final_value\n",
    "        for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "            R = r + gamma * R * (1 - d)\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def train_epoch(self, num_episodes: int = 100):\n",
    "        total_loss = 0.0\n",
    "        total_episodes = 0\n",
    "\n",
    "        for _ in range(num_episodes):\n",
    "            # Select opponent: 50% current, 50% past\n",
    "            if len(self.past_policies) > 0 and random.random() < 0.5:\n",
    "                opponent = random.choice(self.past_policies)\n",
    "                agent_is_x = random.choice([True, False])\n",
    "            else:\n",
    "                opponent = None\n",
    "                agent_is_x = True\n",
    "\n",
    "            env = GomokuEnv(board_size=self.board_size, agent_is_x=agent_is_x)\n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "            entropies = []\n",
    "            dones_list = []\n",
    "\n",
    "            while not done:\n",
    "                valid_mask = env.get_valid_mask()\n",
    "                action, log_prob, value, entropy = self.get_action(state, valid_mask)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                rewards.append(reward)\n",
    "                entropies.append(entropy)\n",
    "                dones_list.append(float(done))\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Bootstrap final value\n",
    "            if done:\n",
    "                final_value = 0.0\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                _, final_value = self.policy(state_tensor)\n",
    "                final_value = final_value.item()\n",
    "\n",
    "            returns = self.compute_returns(rewards, dones_list, final_value, self.gamma)\n",
    "            returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "            values = torch.cat(values)\n",
    "            advantages = returns - values\n",
    "\n",
    "            log_probs = torch.cat(log_probs)\n",
    "            entropies = torch.cat(entropies)\n",
    "\n",
    "            actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "            critic_loss = advantages.pow(2).mean()\n",
    "            entropy_loss = -entropies.mean()\n",
    "\n",
    "            loss = actor_loss + self.value_coef * critic_loss + self.entropy_coef * entropy_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_episodes += 1\n",
    "\n",
    "        return total_loss / total_episodes\n",
    "\n",
    "    def save_policy(self, path: str):\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "\n",
    "    def add_past_policy(self):\n",
    "        self.past_policies.append(self._copy_policy())\n",
    "\n",
    "    def _copy_policy(self):\n",
    "        policy_copy = PolicyNetwork(self.board_size).to(self.device)\n",
    "        policy_copy.load_state_dict(self.policy.state_dict())\n",
    "        policy_copy.eval()\n",
    "        return policy_copy"
   ],
   "id": "901a885bf2c77f6b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:02:22.897938647Z",
     "start_time": "2026-01-01T12:02:22.865202402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_policy(policy, board_size: int = 15, episodes: int = 100):\n",
    "    wins, draws, losses = 0, 0, 0\n",
    "    device = next(policy.parameters()).device\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        agent_is_x = random.choice([True, False])\n",
    "        env = GomokuEnv(board_size=board_size, agent_is_x=agent_is_x)\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            valid_mask = env.get_valid_mask()\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            valid_mask_tensor = torch.tensor(valid_mask, dtype=torch.bool).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, _ = policy(state_tensor)\n",
    "                masked_logits = logits.masked_fill(~valid_mask_tensor, -1e9)\n",
    "                action = masked_logits.argmax(dim=-1).item()\n",
    "\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "        if reward == 1.0:\n",
    "            wins += 1\n",
    "        elif reward == 0.0:\n",
    "            draws += 1\n",
    "        else:\n",
    "            losses += 1\n",
    "\n",
    "    return wins / episodes, draws / episodes, losses / episodes"
   ],
   "id": "5ae43cda31bbd341",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T13:23:34.361488076Z",
     "start_time": "2026-01-01T13:20:17.876587274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = SelfPlayA2C(board_size=15, lr=1e-4)\n",
    "device = trainer.device\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Elo setup\n",
    "elo = EloEvaluator(k_factor=20.0)\n",
    "elo.add_player(\"current\", 1200.0)\n",
    "\n",
    "past_checkpoints = []\n",
    "\n",
    "EVAL_FREQ = 20      # episodes\n",
    "ELO_FREQ = 100      # episodes\n",
    "TOTAL_EPISODES = 2000\n",
    "\n",
    "for episode in range(TOTAL_EPISODES):\n",
    "    loss = trainer.train_epoch(num_episodes=1)  # 1 episode per step\n",
    "\n",
    "    # --- Periodic Evaluation ---\n",
    "    if episode % EVAL_FREQ == 0:\n",
    "        # Evaluate vs random\n",
    "        win_r, draw_r, loss_r = evaluate_agent(\n",
    "            trainer.policy,\n",
    "            lambda env: random_opponent(env),\n",
    "            board_size=15,\n",
    "            episodes=50,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Evaluate vs heuristic\n",
    "        win_h, draw_h, loss_h = evaluate_agent(\n",
    "            trainer.policy,\n",
    "            lambda env: heuristic_opponent(env),\n",
    "            board_size=15,\n",
    "            episodes=50,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Ep {episode:4d} | \"\n",
    "            f\"Loss: {loss:7.4f} | \"\n",
    "            f\"Rand: W{win_r:5.1%} D{draw_r:5.1%} L{loss_r:5.1%} | \"\n",
    "            f\"Heur: W{win_h:5.1%} D{draw_h:5.1%} L{loss_h:5.1%} | \"\n",
    "            f\"ε: {trainer.entropy_coef:.3f}\"\n",
    "        )\n",
    "\n",
    "    # --- Elo Evaluation vs Past Models ---\n",
    "    if episode % ELO_FREQ == 0 and episode > 0:\n",
    "        # Save current as checkpoint\n",
    "        ckpt_path = f\"models/gomoku_ep_{episode:04d}.pth\"\n",
    "        torch.save(trainer.policy.state_dict(), ckpt_path)\n",
    "        past_checkpoints.append(ckpt_path)\n",
    "\n",
    "        # Only keep last 5 for efficiency\n",
    "        if len(past_checkpoints) > 5:\n",
    "            oldest = past_checkpoints.pop(0)\n",
    "            if os.path.exists(oldest):\n",
    "                os.remove(oldest)\n",
    "\n",
    "        # Evaluate against all past checkpoints\n",
    "        current_rating = elo.get_rating(\"current\")\n",
    "        for past_path in past_checkpoints[:-1]:  # exclude self\n",
    "            name = os.path.basename(past_path).replace(\".pth\", \"\")\n",
    "            if name not in elo.ratings:\n",
    "                elo.add_player(name, 1200.0)\n",
    "\n",
    "            opp_policy = PolicyNetwork(board_size=15).to(device)\n",
    "            opp_policy.load_state_dict(torch.load(past_path, map_location=device))\n",
    "            opp_policy.eval()\n",
    "\n",
    "            wins = 0\n",
    "            eval_eps = 20  # faster Elo\n",
    "            for _ in range(eval_eps):\n",
    "                agent_is_x = random.choice([True, False])\n",
    "                env = GomokuEnv(board_size=15, agent_is_x=agent_is_x)\n",
    "                obs, _ = env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    if env.agent_is_x == agent_is_x:\n",
    "                        # current agent\n",
    "                        valid_mask = (env.board.flatten() == 0)\n",
    "                        s = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                        vm = torch.tensor(valid_mask, dtype=torch.bool).unsqueeze(0).to(device)\n",
    "                        with torch.no_grad():\n",
    "                            logits, _ = trainer.policy(s)\n",
    "                            action = logits.masked_fill(~vm, -1e9).argmax().item()\n",
    "                        obs, r, term, trunc, _ = env.step(action)\n",
    "                    else:\n",
    "                        # past agent\n",
    "                        valid_mask = (env.board.flatten() == 0)\n",
    "                        s = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                        vm = torch.tensor(valid_mask, dtype=torch.bool).unsqueeze(0).to(device)\n",
    "                        with torch.no_grad():\n",
    "                            logits, _ = opp_policy(s)\n",
    "                            action = logits.masked_fill(~vm, -1e9).argmax().item()\n",
    "                        obs, r, term, trunc, _ = env.step(action)\n",
    "                    done = term or trunc\n",
    "                if (agent_is_x and r == 1.0) or (not agent_is_x and r == -1.0):\n",
    "                    wins += 1\n",
    "\n",
    "            score = wins / eval_eps\n",
    "            elo.update_rating(\"current\", name, score)\n",
    "\n",
    "        current_rating = elo.get_rating(\"current\")\n",
    "        print(f\"Ep {episode:4d} | Elo: {current_rating:6.1f} (vs {len(past_checkpoints)-1} past models)\")\n",
    "\n",
    "    # --- Add to self-play pool periodically ---\n",
    "    if episode % 50 == 0:\n",
    "        trainer.add_past_policy()"
   ],
   "id": "80b44e9da3a299e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep    0 | Loss:  3.9982 | Rand: W46.0% D 0.0% L54.0% | Heur: W50.0% D 0.0% L50.0% | ε: 0.010\n",
      "Ep   20 | Loss: -3.9414 | Rand: W46.0% D 0.0% L54.0% | Heur: W58.0% D 0.0% L42.0% | ε: 0.010\n",
      "Ep   40 | Loss:  3.9171 | Rand: W46.0% D 0.0% L54.0% | Heur: W50.0% D 0.0% L50.0% | ε: 0.010\n",
      "Ep   60 | Loss:  4.4260 | Rand: W44.0% D 0.0% L56.0% | Heur: W50.0% D 0.0% L50.0% | ε: 0.010\n",
      "Ep   80 | Loss: -3.7417 | Rand: W48.0% D 0.0% L52.0% | Heur: W60.0% D 0.0% L40.0% | ε: 0.010\n",
      "Ep  100 | Loss:  3.8861 | Rand: W62.0% D 0.0% L38.0% | Heur: W62.0% D 0.0% L38.0% | ε: 0.010\n",
      "Ep  100 | Elo: 1200.0 (vs 0 past models)\n",
      "Ep  120 | Loss:  3.9477 | Rand: W48.0% D 0.0% L52.0% | Heur: W52.0% D 0.0% L48.0% | ε: 0.010\n",
      "Ep  140 | Loss: -4.1917 | Rand: W58.0% D 0.0% L42.0% | Heur: W56.0% D 0.0% L44.0% | ε: 0.010\n",
      "Ep  160 | Loss:  3.6548 | Rand: W52.0% D 0.0% L48.0% | Heur: W54.0% D 0.0% L46.0% | ε: 0.010\n",
      "Ep  180 | Loss: -4.2723 | Rand: W54.0% D 0.0% L46.0% | Heur: W44.0% D 0.0% L56.0% | ε: 0.010\n",
      "Ep  200 | Loss:  3.9486 | Rand: W54.0% D 0.0% L46.0% | Heur: W54.0% D 0.0% L46.0% | ε: 0.010\n",
      "Ep  200 | Elo: 1200.0 (vs 1 past models)\n",
      "Ep  220 | Loss: -3.9453 | Rand: W48.0% D 0.0% L52.0% | Heur: W62.0% D 0.0% L38.0% | ε: 0.010\n",
      "Ep  240 | Loss:  4.6978 | Rand: W56.0% D 0.0% L44.0% | Heur: W58.0% D 0.0% L42.0% | ε: 0.010\n",
      "Ep  260 | Loss:  2.4904 | Rand: W56.0% D 0.0% L44.0% | Heur: W54.0% D 0.0% L46.0% | ε: 0.010\n",
      "Ep  280 | Loss:  2.3367 | Rand: W62.0% D 0.0% L38.0% | Heur: W58.0% D 0.0% L42.0% | ε: 0.010\n",
      "Ep  300 | Loss: -4.3205 | Rand: W48.0% D 0.0% L52.0% | Heur: W44.0% D 0.0% L56.0% | ε: 0.010\n",
      "Ep  300 | Elo: 1203.0 (vs 2 past models)\n",
      "Ep  320 | Loss: -4.2754 | Rand: W44.0% D 0.0% L56.0% | Heur: W60.0% D 0.0% L40.0% | ε: 0.010\n",
      "Ep  340 | Loss:  3.3385 | Rand: W50.0% D 0.0% L50.0% | Heur: W42.0% D 0.0% L58.0% | ε: 0.010\n",
      "Ep  360 | Loss: -3.7429 | Rand: W64.0% D 0.0% L36.0% | Heur: W52.0% D 0.0% L48.0% | ε: 0.010\n",
      "Ep  380 | Loss: -3.8791 | Rand: W42.0% D 0.0% L58.0% | Heur: W32.0% D 0.0% L68.0% | ε: 0.010\n",
      "Ep  400 | Loss: -4.1732 | Rand: W56.0% D 0.0% L44.0% | Heur: W46.0% D 0.0% L54.0% | ε: 0.010\n",
      "Ep  400 | Elo: 1197.8 (vs 3 past models)\n",
      "Ep  420 | Loss: -3.7656 | Rand: W44.0% D 0.0% L56.0% | Heur: W52.0% D 0.0% L48.0% | ε: 0.010\n",
      "Ep  440 | Loss:  3.4808 | Rand: W54.0% D 0.0% L46.0% | Heur: W42.0% D 0.0% L58.0% | ε: 0.010\n",
      "Ep  460 | Loss: -3.4724 | Rand: W42.0% D 0.0% L58.0% | Heur: W44.0% D 0.0% L56.0% | ε: 0.010\n",
      "Ep  480 | Loss:  3.2591 | Rand: W62.0% D 0.0% L38.0% | Heur: W54.0% D 0.0% L46.0% | ε: 0.010\n",
      "Ep  500 | Loss: -3.3428 | Rand: W52.0% D 0.0% L48.0% | Heur: W50.0% D 0.0% L50.0% | ε: 0.010\n",
      "Ep  500 | Elo: 1203.1 (vs 4 past models)\n",
      "Ep  520 | Loss:  4.3071 | Rand: W44.0% D 0.0% L56.0% | Heur: W42.0% D 0.0% L58.0% | ε: 0.010\n",
      "Ep  540 | Loss:  3.9973 | Rand: W56.0% D 0.0% L44.0% | Heur: W48.0% D 0.0% L52.0% | ε: 0.010\n",
      "Ep  560 | Loss: -4.4270 | Rand: W38.0% D 0.0% L62.0% | Heur: W58.0% D 0.0% L42.0% | ε: 0.010\n",
      "Ep  580 | Loss: -3.9145 | Rand: W52.0% D 0.0% L48.0% | Heur: W54.0% D 0.0% L46.0% | ε: 0.010\n",
      "Ep  600 | Loss:  3.8373 | Rand: W44.0% D 0.0% L56.0% | Heur: W56.0% D 0.0% L44.0% | ε: 0.010\n",
      "Ep  600 | Elo: 1200.5 (vs 4 past models)\n",
      "Ep  620 | Loss: -3.8865 | Rand: W52.0% D 0.0% L48.0% | Heur: W48.0% D 0.0% L52.0% | ε: 0.010\n",
      "Ep  640 | Loss: -3.5888 | Rand: W46.0% D 0.0% L54.0% | Heur: W52.0% D 0.0% L48.0% | ε: 0.010\n",
      "Ep  660 | Loss: -4.1805 | Rand: W66.0% D 0.0% L34.0% | Heur: W62.0% D 0.0% L38.0% | ε: 0.010\n",
      "Ep  680 | Loss: -3.5524 | Rand: W64.0% D 0.0% L36.0% | Heur: W52.0% D 0.0% L48.0% | ε: 0.010\n",
      "Ep  700 | Loss: -3.1398 | Rand: W78.0% D 0.0% L22.0% | Heur: W72.0% D 0.0% L28.0% | ε: 0.010\n",
      "Ep  700 | Elo: 1205.4 (vs 4 past models)\n",
      "Ep  720 | Loss: -3.4665 | Rand: W54.0% D 0.0% L46.0% | Heur: W72.0% D 0.0% L28.0% | ε: 0.010\n",
      "Ep  740 | Loss: -3.5787 | Rand: W72.0% D 0.0% L28.0% | Heur: W72.0% D 0.0% L28.0% | ε: 0.010\n",
      "Ep  760 | Loss: -3.4759 | Rand: W78.0% D 0.0% L22.0% | Heur: W78.0% D 0.0% L22.0% | ε: 0.010\n",
      "Ep  780 | Loss:  3.8328 | Rand: W78.0% D 0.0% L22.0% | Heur: W80.0% D 0.0% L20.0% | ε: 0.010\n",
      "Ep  800 | Loss:  3.9279 | Rand: W84.0% D 0.0% L16.0% | Heur: W78.0% D 0.0% L22.0% | ε: 0.010\n",
      "Ep  800 | Elo: 1209.6 (vs 4 past models)\n",
      "Ep  820 | Loss: -3.6065 | Rand: W84.0% D 0.0% L16.0% | Heur: W84.0% D 0.0% L16.0% | ε: 0.010\n",
      "Ep  840 | Loss:  4.4170 | Rand: W76.0% D 0.0% L24.0% | Heur: W84.0% D 0.0% L16.0% | ε: 0.010\n",
      "Ep  860 | Loss: -3.2758 | Rand: W72.0% D 0.0% L28.0% | Heur: W78.0% D 0.0% L22.0% | ε: 0.010\n",
      "Ep  880 | Loss: -4.0079 | Rand: W78.0% D 0.0% L22.0% | Heur: W84.0% D 0.0% L16.0% | ε: 0.010\n",
      "Ep  900 | Loss: -3.0135 | Rand: W80.0% D 0.0% L20.0% | Heur: W88.0% D 0.0% L12.0% | ε: 0.010\n",
      "Ep  900 | Elo: 1208.3 (vs 4 past models)\n",
      "Ep  920 | Loss: -2.9801 | Rand: W90.0% D 0.0% L10.0% | Heur: W90.0% D 0.0% L10.0% | ε: 0.010\n",
      "Ep  940 | Loss: -3.3931 | Rand: W84.0% D 0.0% L16.0% | Heur: W80.0% D 0.0% L20.0% | ε: 0.010\n",
      "Ep  960 | Loss:  4.9333 | Rand: W82.0% D 0.0% L18.0% | Heur: W84.0% D 0.0% L16.0% | ε: 0.010\n",
      "Ep  980 | Loss: -3.9031 | Rand: W72.0% D 0.0% L28.0% | Heur: W76.0% D 0.0% L24.0% | ε: 0.010\n",
      "Ep 1000 | Loss: -3.8433 | Rand: W62.0% D 0.0% L38.0% | Heur: W78.0% D 0.0% L22.0% | ε: 0.010\n",
      "Ep 1000 | Elo: 1205.3 (vs 4 past models)\n",
      "Ep 1020 | Loss:  4.3105 | Rand: W82.0% D 0.0% L18.0% | Heur: W88.0% D 0.0% L12.0% | ε: 0.010\n",
      "Ep 1040 | Loss:  5.4666 | Rand: W82.0% D 0.0% L18.0% | Heur: W82.0% D 0.0% L18.0% | ε: 0.010\n",
      "Ep 1060 | Loss: -3.4738 | Rand: W74.0% D 0.0% L26.0% | Heur: W82.0% D 0.0% L18.0% | ε: 0.010\n",
      "Ep 1080 | Loss: -3.2192 | Rand: W74.0% D 0.0% L26.0% | Heur: W76.0% D 0.0% L24.0% | ε: 0.010\n",
      "Ep 1100 | Loss: -3.6146 | Rand: W60.0% D 0.0% L40.0% | Heur: W78.0% D 0.0% L22.0% | ε: 0.010\n",
      "Ep 1100 | Elo: 1209.4 (vs 4 past models)\n",
      "Ep 1120 | Loss:  4.4529 | Rand: W72.0% D 0.0% L28.0% | Heur: W70.0% D 0.0% L30.0% | ε: 0.010\n",
      "Ep 1140 | Loss:  4.3910 | Rand: W66.0% D 0.0% L34.0% | Heur: W66.0% D 0.0% L34.0% | ε: 0.010\n",
      "Ep 1160 | Loss: -3.3215 | Rand: W60.0% D 0.0% L40.0% | Heur: W66.0% D 0.0% L34.0% | ε: 0.010\n",
      "Ep 1180 | Loss:  4.7478 | Rand: W76.0% D 0.0% L24.0% | Heur: W52.0% D 0.0% L48.0% | ε: 0.010\n",
      "Ep 1200 | Loss:  4.4551 | Rand: W56.0% D 0.0% L44.0% | Heur: W66.0% D 0.0% L34.0% | ε: 0.010\n",
      "Ep 1200 | Elo: 1212.2 (vs 4 past models)\n",
      "Ep 1220 | Loss:  4.3932 | Rand: W70.0% D 0.0% L30.0% | Heur: W74.0% D 0.0% L26.0% | ε: 0.010\n",
      "Ep 1240 | Loss:  3.9474 | Rand: W58.0% D 0.0% L42.0% | Heur: W64.0% D 0.0% L36.0% | ε: 0.010\n",
      "Ep 1260 | Loss:  2.8788 | Rand: W68.0% D 0.0% L32.0% | Heur: W54.0% D 0.0% L46.0% | ε: 0.010\n",
      "Ep 1280 | Loss:  3.0944 | Rand: W72.0% D 0.0% L28.0% | Heur: W70.0% D 0.0% L30.0% | ε: 0.010\n",
      "Ep 1300 | Loss: -4.1393 | Rand: W62.0% D 0.0% L38.0% | Heur: W64.0% D 0.0% L36.0% | ε: 0.010\n",
      "Ep 1300 | Elo: 1218.6 (vs 4 past models)\n",
      "Ep 1320 | Loss: -3.9079 | Rand: W50.0% D 0.0% L50.0% | Heur: W72.0% D 0.0% L28.0% | ε: 0.010\n",
      "Ep 1340 | Loss:  2.9690 | Rand: W58.0% D 0.0% L42.0% | Heur: W56.0% D 0.0% L44.0% | ε: 0.010\n",
      "Ep 1360 | Loss: -3.7168 | Rand: W52.0% D 0.0% L48.0% | Heur: W50.0% D 0.0% L50.0% | ε: 0.010\n",
      "Ep 1380 | Loss:  3.7470 | Rand: W52.0% D 0.0% L48.0% | Heur: W52.0% D 0.0% L48.0% | ε: 0.010\n",
      "Ep 1400 | Loss: -4.4068 | Rand: W68.0% D 0.0% L32.0% | Heur: W46.0% D 0.0% L54.0% | ε: 0.010\n",
      "Ep 1400 | Elo: 1219.2 (vs 4 past models)\n",
      "Ep 1420 | Loss: -4.4152 | Rand: W52.0% D 0.0% L48.0% | Heur: W48.0% D 0.0% L52.0% | ε: 0.010\n",
      "Ep 1440 | Loss: -5.1243 | Rand: W56.0% D 0.0% L44.0% | Heur: W64.0% D 0.0% L36.0% | ε: 0.010\n",
      "Ep 1460 | Loss: -4.1298 | Rand: W54.0% D 0.0% L46.0% | Heur: W36.0% D 0.0% L64.0% | ε: 0.010\n",
      "Ep 1480 | Loss:  3.6979 | Rand: W42.0% D 0.0% L58.0% | Heur: W52.0% D 0.0% L48.0% | ε: 0.010\n",
      "Ep 1500 | Loss:  2.9284 | Rand: W62.0% D 0.0% L38.0% | Heur: W46.0% D 0.0% L54.0% | ε: 0.010\n",
      "Ep 1500 | Elo: 1229.5 (vs 4 past models)\n",
      "Ep 1520 | Loss: -5.2078 | Rand: W60.0% D 0.0% L40.0% | Heur: W54.0% D 0.0% L46.0% | ε: 0.010\n",
      "Ep 1540 | Loss:  2.7164 | Rand: W62.0% D 0.0% L38.0% | Heur: W68.0% D 0.0% L32.0% | ε: 0.010\n",
      "Ep 1560 | Loss: -6.1581 | Rand: W64.0% D 0.0% L36.0% | Heur: W64.0% D 0.0% L36.0% | ε: 0.010\n",
      "Ep 1580 | Loss:  2.9361 | Rand: W60.0% D 0.0% L40.0% | Heur: W62.0% D 0.0% L38.0% | ε: 0.010\n",
      "Ep 1600 | Loss:  2.6295 | Rand: W68.0% D 0.0% L32.0% | Heur: W70.0% D 0.0% L30.0% | ε: 0.010\n",
      "Ep 1600 | Elo: 1225.3 (vs 4 past models)\n",
      "Ep 1620 | Loss: -5.1683 | Rand: W68.0% D 0.0% L32.0% | Heur: W62.0% D 0.0% L38.0% | ε: 0.010\n",
      "Ep 1640 | Loss:  0.8948 | Rand: W64.0% D 0.0% L36.0% | Heur: W64.0% D 0.0% L36.0% | ε: 0.010\n",
      "Ep 1660 | Loss: -5.3221 | Rand: W66.0% D 0.0% L34.0% | Heur: W78.0% D 0.0% L22.0% | ε: 0.010\n",
      "Ep 1680 | Loss:  0.8296 | Rand: W70.0% D 0.0% L30.0% | Heur: W74.0% D 0.0% L26.0% | ε: 0.010\n",
      "Ep 1700 | Loss:  0.8439 | Rand: W32.0% D 0.0% L68.0% | Heur: W58.0% D 0.0% L42.0% | ε: 0.010\n",
      "Ep 1700 | Elo: 1222.2 (vs 4 past models)\n",
      "Ep 1720 | Loss:  1.9628 | Rand: W60.0% D 0.0% L40.0% | Heur: W56.0% D 0.0% L44.0% | ε: 0.010\n",
      "Ep 1740 | Loss:  0.2871 | Rand: W58.0% D 0.0% L42.0% | Heur: W54.0% D 0.0% L46.0% | ε: 0.010\n",
      "Ep 1760 | Loss:  0.8040 | Rand: W46.0% D 0.0% L54.0% | Heur: W54.0% D 0.0% L46.0% | ε: 0.010\n",
      "Ep 1780 | Loss:  1.7380 | Rand: W54.0% D 0.0% L46.0% | Heur: W60.0% D 0.0% L40.0% | ε: 0.010\n",
      "Ep 1800 | Loss: -5.5090 | Rand: W40.0% D 0.0% L60.0% | Heur: W54.0% D 0.0% L46.0% | ε: 0.010\n",
      "Ep 1800 | Elo: 1215.8 (vs 4 past models)\n",
      "Ep 1820 | Loss: -5.6608 | Rand: W62.0% D 0.0% L38.0% | Heur: W50.0% D 0.0% L50.0% | ε: 0.010\n",
      "Ep 1840 | Loss: -5.8152 | Rand: W40.0% D 0.0% L60.0% | Heur: W42.0% D 0.0% L58.0% | ε: 0.010\n",
      "Ep 1860 | Loss:  1.8377 | Rand: W50.0% D 0.0% L50.0% | Heur: W62.0% D 0.0% L38.0% | ε: 0.010\n",
      "Ep 1880 | Loss: -5.3846 | Rand: W44.0% D 0.0% L56.0% | Heur: W52.0% D 0.0% L48.0% | ε: 0.010\n",
      "Ep 1900 | Loss:  1.4320 | Rand: W52.0% D 0.0% L48.0% | Heur: W50.0% D 0.0% L50.0% | ε: 0.010\n",
      "Ep 1900 | Elo: 1216.4 (vs 4 past models)\n",
      "Ep 1920 | Loss:  1.5655 | Rand: W50.0% D 0.0% L50.0% | Heur: W56.0% D 0.0% L44.0% | ε: 0.010\n",
      "Ep 1940 | Loss: -5.7691 | Rand: W56.0% D 0.0% L44.0% | Heur: W60.0% D 0.0% L40.0% | ε: 0.010\n",
      "Ep 1960 | Loss: -5.5546 | Rand: W62.0% D 0.0% L38.0% | Heur: W52.0% D 0.0% L48.0% | ε: 0.010\n",
      "Ep 1980 | Loss: -6.0181 | Rand: W42.0% D 0.0% L58.0% | Heur: W56.0% D 0.0% L44.0% | ε: 0.010\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Play against the bot",
   "id": "4a8330b74e98f969"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T13:31:43.279523528Z",
     "start_time": "2026-01-01T13:31:30.755718523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "BOARD_SIZE = 15\n",
    "CELL_SIZE = 30  # pixels\n",
    "WINDOW_SIZE = BOARD_SIZE * CELL_SIZE\n",
    "MODEL_PATH = \"models/gomoku_ep_1900.pth\"\n",
    "\n",
    "# ----------------------------\n",
    "# Load agent\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy = PolicyNetwork(board_size=BOARD_SIZE).to(device)\n",
    "policy.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "policy.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# Game state\n",
    "# ----------------------------\n",
    "env = GomokuEnv(board_size=BOARD_SIZE, agent_is_x=True)\n",
    "obs, _ = env.reset()\n",
    "game_over = False\n",
    "message = \"Click to place O (you play second). Press R to reset, Q to quit.\"\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: check win\n",
    "# ----------------------------\n",
    "def check_win(board, r, c):\n",
    "    if board[r, c] == 0:\n",
    "        return 0\n",
    "    player = board[r, c]\n",
    "    for dr, dc in [(0,1), (1,0), (1,1), (1,-1)]:\n",
    "        count = 1\n",
    "        for sign in [1, -1]:\n",
    "            nr, nc = r + sign*dr, c + sign*dc\n",
    "            while 0 <= nr < BOARD_SIZE and 0 <= nc < BOARD_SIZE and board[nr, nc] == player:\n",
    "                count += 1\n",
    "                nr += sign*dr\n",
    "                nc += sign*dc\n",
    "        if count >= 5:\n",
    "            return player\n",
    "    return 0\n",
    "\n",
    "# ----------------------------\n",
    "# Draw board\n",
    "# ----------------------------\n",
    "def draw_board(img, board, message):\n",
    "    img[:] = 255  # white background\n",
    "\n",
    "    # Draw grid\n",
    "    for i in range(1, BOARD_SIZE):\n",
    "        cv2.line(img, (i * CELL_SIZE, 0), (i * CELL_SIZE, WINDOW_SIZE), (200, 200, 200), 1)\n",
    "        cv2.line(img, (0, i * CELL_SIZE), (WINDOW_SIZE, i * CELL_SIZE), (200, 200, 200), 1)\n",
    "\n",
    "    # Draw stones\n",
    "    for i in range(BOARD_SIZE):\n",
    "        for j in range(BOARD_SIZE):\n",
    "            center = (j * CELL_SIZE + CELL_SIZE // 2, i * CELL_SIZE + CELL_SIZE // 2)\n",
    "            if board[i, j] == 1:  # X (agent)\n",
    "                cv2.circle(img, center, CELL_SIZE // 2 - 2, (0, 0, 0), -1)\n",
    "            elif board[i, j] == -1:  # O (you)\n",
    "                cv2.circle(img, center, CELL_SIZE // 2 - 2, (255, 255, 255), -1)\n",
    "                cv2.circle(img, center, CELL_SIZE // 2 - 2, (0, 0, 0), 1)\n",
    "\n",
    "    # Draw message\n",
    "    cv2.putText(img, message, (10, WINDOW_SIZE + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
    "\n",
    "# ----------------------------\n",
    "# Main loop\n",
    "# ----------------------------\n",
    "img = np.ones((WINDOW_SIZE + 30, WINDOW_SIZE, 3), dtype=np.uint8) * 255\n",
    "cv2.namedWindow(\"Gomoku - Play vs A2C Agent\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global obs, game_over, message\n",
    "    if event == cv2.EVENT_LBUTTONDOWN and not game_over:\n",
    "        if y >= WINDOW_SIZE:  # clicked on message area\n",
    "            return\n",
    "        col = x // CELL_SIZE\n",
    "        row = y // CELL_SIZE\n",
    "        if not (0 <= row < BOARD_SIZE and 0 <= col < BOARD_SIZE):\n",
    "            return\n",
    "        if env.board[row, col] != 0:\n",
    "            message = \"Invalid: cell occupied.\"\n",
    "            return\n",
    "\n",
    "        # Human move\n",
    "        env.board[row, col] = -1\n",
    "        winner = check_win(env.board, row, col)\n",
    "        full = not (env.board == 0).any()\n",
    "\n",
    "        if winner == -1:\n",
    "            message = \"You win! Press R to reset.\"\n",
    "            game_over = True\n",
    "        elif full:\n",
    "            message = \"Draw! Press R to reset.\"\n",
    "            game_over = True\n",
    "        else:\n",
    "            # Agent move\n",
    "            valid_mask = (env.board.flatten() == 0)\n",
    "            state_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            valid_mask_tensor = torch.tensor(valid_mask, dtype=torch.bool).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                logits, _ = policy(state_tensor)\n",
    "                masked_logits = logits.masked_fill(~valid_mask_tensor, -1e9)\n",
    "                action = masked_logits.argmax(dim=-1).item()\n",
    "            ar, ac = divmod(action, BOARD_SIZE)\n",
    "            env.board[ar, ac] = 1\n",
    "            obs = env._get_obs()\n",
    "            winner = check_win(env.board, ar, ac)\n",
    "            full = not (env.board == 0).any()\n",
    "            if winner == 1:\n",
    "                message = \"Agent wins! Press R to reset.\"\n",
    "                game_over = True\n",
    "            elif full:\n",
    "                message = \"Draw! Press R to reset.\"\n",
    "                game_over = True\n",
    "            else:\n",
    "                message = \"Your turn: click to place O.\"\n",
    "\n",
    "        draw_board(img, env.board, message)\n",
    "        cv2.imshow(\"Gomoku - Play vs A2C Agent\", img)\n",
    "\n",
    "cv2.setMouseCallback(\"Gomoku - Play vs A2C Agent\", mouse_callback)\n",
    "draw_board(img, env.board, message)\n",
    "cv2.imshow(\"Gomoku - Play vs A2C Agent\", img)\n",
    "\n",
    "print(\"Game started. Click to play. Press R to reset, Q to quit.\")\n",
    "\n",
    "while True:\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q') or cv2.getWindowProperty(\"Gomoku - Play vs A2C Agent\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "    if key == ord('r'):\n",
    "        env.reset()\n",
    "        obs, _ = env.reset()\n",
    "        game_over = False\n",
    "        message = \"Click to place O (you play second).\"\n",
    "        draw_board(img, env.board, message)\n",
    "        cv2.imshow(\"Gomoku - Play vs A2C Agent\", img)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "f0a63b3d4509ecc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game started. Click to play. Press R to reset, Q to quit.\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T13:16:24.311413173Z",
     "start_time": "2026-01-01T13:16:24.285727002Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f2993ff86ee96c32",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T13:16:49.719060567Z",
     "start_time": "2026-01-01T13:16:47.679470603Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a1a6c0551a36b9c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Opponent:   Win=100.00%, Draw=0.00%, Loss=0.00%\n",
      "Heuristic Opponent: Win=100.00%, Draw=0.00%, Loss=0.00%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PolicyNetwork:\n\tMissing key(s) in state_dict: \"actor.1.weight\", \"actor.1.bias\", \"actor.3.weight\", \"actor.3.bias\", \"actor.5.weight\", \"actor.5.bias\", \"critic.1.weight\", \"critic.1.bias\", \"critic.3.weight\", \"critic.3.bias\", \"critic.5.weight\", \"critic.5.bias\". \n\tUnexpected key(s) in state_dict: \"net.1.weight\", \"net.1.bias\", \"net.3.weight\", \"net.3.bias\", \"net.5.weight\", \"net.5.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mrun_full_evaluation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpolicy_path\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodels/gomoku_a2c_final.pth\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_models_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodels\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mboard_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m15\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepisodes_per_match\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m100\u001B[39;49m\n\u001B[32m      6\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[34]\u001B[39m\u001B[32m, line 217\u001B[39m, in \u001B[36mrun_full_evaluation\u001B[39m\u001B[34m(policy_path, past_models_dir, board_size, episodes_per_match, device)\u001B[39m\n\u001B[32m    215\u001B[39m \u001B[38;5;66;03m# Load opponent policy\u001B[39;00m\n\u001B[32m    216\u001B[39m opp_policy = PolicyNetwork(board_size=board_size).to(device)\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m \u001B[43mopp_policy\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpast_models_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_file\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    218\u001B[39m opp_policy.eval()\n\u001B[32m    220\u001B[39m \u001B[38;5;66;03m# Play matches\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/projects/play-TicTacToe/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2629\u001B[39m, in \u001B[36mModule.load_state_dict\u001B[39m\u001B[34m(self, state_dict, strict, assign)\u001B[39m\n\u001B[32m   2621\u001B[39m         error_msgs.insert(\n\u001B[32m   2622\u001B[39m             \u001B[32m0\u001B[39m,\n\u001B[32m   2623\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m. \u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m   2624\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m.join(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)\n\u001B[32m   2625\u001B[39m             ),\n\u001B[32m   2626\u001B[39m         )\n\u001B[32m   2628\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) > \u001B[32m0\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m2629\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   2630\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(\n\u001B[32m   2631\u001B[39m             \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m, \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[33m\"\u001B[39m.join(error_msgs)\n\u001B[32m   2632\u001B[39m         )\n\u001B[32m   2633\u001B[39m     )\n\u001B[32m   2634\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[31mRuntimeError\u001B[39m: Error(s) in loading state_dict for PolicyNetwork:\n\tMissing key(s) in state_dict: \"actor.1.weight\", \"actor.1.bias\", \"actor.3.weight\", \"actor.3.bias\", \"actor.5.weight\", \"actor.5.bias\", \"critic.1.weight\", \"critic.1.bias\", \"critic.3.weight\", \"critic.3.bias\", \"critic.5.weight\", \"critic.5.bias\". \n\tUnexpected key(s) in state_dict: \"net.1.weight\", \"net.1.bias\", \"net.3.weight\", \"net.3.bias\", \"net.5.weight\", \"net.5.bias\". "
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c6d1f878d5f561a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
